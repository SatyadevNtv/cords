{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "classify.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4GHKocGOGwz"
      },
      "source": [
        "Import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RonfMxbVOm83"
      },
      "source": [
        "import argparse\n",
        "import copy\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import recall_score, precision_score\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Subset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from cords.selectionstrategies.supervisedlearning import CRAIGStrategy\n",
        "from dataloader import ChestXRayImageData\n",
        "from arch import XRayNet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYAL8aw5Ou0Q"
      },
      "source": [
        "Set random seeds for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE-eMxP6O3DQ"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDctolv9O6I5"
      },
      "source": [
        "Helper functions specific to main process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K79NL2gvPB7L"
      },
      "source": [
        "def get_acc(model, dloader):\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        ypred = []\n",
        "        ytruth = []\n",
        "        for data in dloader:\n",
        "            images, labels = data['image'], data['label']\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(torch.nn.functional.log_softmax(outputs.data, dim=1), 1)\n",
        "            ytruth.extend(labels.tolist())\n",
        "            ypred.extend(predicted.tolist())\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Recall: {recall_score(ytruth, ypred)}\")\n",
        "    print(f\"Precision: {precision_score(ytruth, ypred)}\")\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "def train(model, optimizer, objective, dloader, epoch, writer, gammas=None):\n",
        "    running_loss = 0.0\n",
        "    if gammas is not None:\n",
        "        batch_ids = list(dloader.batch_sampler)\n",
        "    for step, data in enumerate(dloader):\n",
        "        imgs, labels = data['image'], data['label']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model(imgs)\n",
        "        loss = objective(pred, labels)\n",
        "        if gammas is not None:\n",
        "            loss = torch.dot(loss, gammas[batch_ids[step]]) / gammas[batch_ids[step]].sum()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if step % 10 == 9:\n",
        "            writer.add_scalar(\"training loss\", running_loss / 10, epoch * len(dloader) + step)\n",
        "            running_loss = 0.0\n",
        "\n",
        "\n",
        "def validate(model, optimizer, objective, dloader, epoch, writer):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    for step, data in enumerate(dloader):\n",
        "        imgs, labels = data['image'], data['label']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model(imgs)\n",
        "        val_loss += objective(pred, labels).mean()\n",
        "    writer.add_scalar(\"validation loss\", val_loss.item() / len(dloader), epoch)\n",
        "    return val_loss\n",
        "\n",
        "\n",
        "def cords(strategy, budget, model, optimizer, objective, tdloader, vdloader):\n",
        "    reflection = copy.deepcopy(model)\n",
        "    start_time = time.time()\n",
        "    subset_idxs, gammas = strategy.select(budget, reflection.state_dict())\n",
        "    print(f\"Subset selection took {time.time() - start_time} secs ..\")\n",
        "    idxs = np.array(subset_idxs)\n",
        "    gammas = np.array(gammas)\n",
        "\n",
        "    if not np.all(gammas):\n",
        "        print(f\"Found zeros in gammas ..\")\n",
        "        nonZeros = np.where(gammas != 0.0)[0]\n",
        "        idxs, gammas = idxs[nonZeros], gammas[nonZeros]\n",
        "\n",
        "    gammas = torch.from_numpy(np.array(gammas)).to(\"cpu\").to(torch.float32)\n",
        "    return idxs, gammas\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hojcb7mdPF6m"
      },
      "source": [
        "Argument parser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyU0KzNJPKYW"
      },
      "source": [
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Pneumonia classifier for ChestXRay Images\")\n",
        "    parser.add_argument('dataset_path', type=str)\n",
        "    parser.add_argument('save_model_path', type=str)\n",
        "    parser.add_argument('--budget', type=float)\n",
        "    parser.add_argument('--no_training', action=\"store_true\")\n",
        "    parser.add_argument('--cords', action=\"store_true\")\n",
        "    parser.add_argument('--verbosity', type=int, default=0)\n",
        "    parser.add_argument('--batch_size', type=int, default=32)\n",
        "    parser.add_argument('--epochs', type=int, default=10)\n",
        "    parser.add_argument('--shuffle', action=\"store_true\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    if args.verbosity:\n",
        "        print(f\"Args: {args}\")\n",
        "    return args\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e1lnnRgPS6n"
      },
      "source": [
        "Main thread entry point"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDDQjmi4PW8W"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    # Tensorboard writer for train/val loss monitoring\n",
        "    writer = SummaryWriter(args.save_model_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6zVPj8APgdN"
      },
      "source": [
        "Instantiate full datasets and corresponding dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-fIPnvUPkzh"
      },
      "source": [
        "    trainDataset = ChestXRayImageData(f\"{args.dataset_path}/train/\")\n",
        "    valDataset = ChestXRayImageData(f\"{args.dataset_path}/val/\")\n",
        "    testDataset = ChestXRayImageData(f\"{args.dataset_path}/test/\")\n",
        "    trainDataloader = torch.utils.data.DataLoader(trainDataset, batch_size=args.batch_size, shuffle=args.shuffle, num_workers=8)\n",
        "    valDataloader = torch.utils.data.DataLoader(valDataset, batch_size=args.batch_size, shuffle=args.shuffle, num_workers=8)\n",
        "    testDataloader = torch.utils.data.DataLoader(testDataset, batch_size=args.batch_size, shuffle=args.shuffle, num_workers=8)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFNWAL2qPor-"
      },
      "source": [
        "Instantiate Model for Classification task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Suz_3U1PsJX"
      },
      "source": [
        "    num_classes = 2\n",
        "    model = XRayNet(3, num_classes)\n",
        "    print(f\"Model summary: {model}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb9-3dJ1PwjY"
      },
      "source": [
        "Instantiate optimizers and schedulers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blv47pjQP0ua"
      },
      "source": [
        "    if not args.no_training:\n",
        "        print(f\"Training for {args.epochs} epochs ...\")\n",
        "\n",
        "        reduction = \"none\" if args.cords else \"mean\"\n",
        "\n",
        "        objective = torch.nn.CrossEntropyLoss(reduction=reduction, weight=torch.FloatTensor([1.95, 0.67]))\n",
        "\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.5)\n",
        "        #optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "        #lrScheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "        #lrScheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=2)\n",
        "\n",
        "        lrScheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
        "\n",
        "        best_loss = float('inf')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG36kM-dP5da"
      },
      "source": [
        "Prepare datasets and dataloaders for CORDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6-3P4ArQHvn"
      },
      "source": [
        "        if args.cords:\n",
        "            shadowModel = copy.deepcopy(model)\n",
        "            strategy = CRAIGStrategy(trainDataloader, valDataloader, shadowModel, objective, \"cpu\", num_classes, False, False, 'PerBatch')\n",
        "            budget = int(args.budget * len(trainDataloader.dataset))\n",
        "            ids = np.random.choice(len(trainDataloader.dataset), size=budget, replace=False)\n",
        "            gammas = torch.ones(len(ids))\n",
        "            trainSubset = Subset(trainDataset, ids)\n",
        "            trainDataLoader = torch.utils.data.DataLoader(trainSubset, batch_size=args.batch_size, shuffle=False)\n",
        "        else:\n",
        "            gammas = None\n",
        "            trainDataLoader = trainDataloader\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLdDbhtAQTXJ"
      },
      "source": [
        "Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkOHpGYaQU9w"
      },
      "source": [
        "        for epoch in range(args.epochs):  # loop over the dataset multiple times\n",
        "            print(f\"Epoch: {epoch}\")\n",
        "\n",
        "            # CORDS\n",
        "            if args.cords and ((epoch + 1) % 5 == 0):\n",
        "                print(f\"Performing CORDS ..\")\n",
        "                ids, gammas = cords(strategy, budget, model, optimizer, objective, trainDataloader, valDataloader)\n",
        "                trainSubset = Subset(trainDataset, ids)\n",
        "                trainDataLoader = torch.utils.data.DataLoader(trainSubset, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "            model.train()\n",
        "            train(model, optimizer, objective, trainDataLoader, epoch, writer, gammas)\n",
        "            val_loss = validate(model, optimizer, objective, valDataloader, epoch, writer)\n",
        "            if best_loss > val_loss:\n",
        "                torch.save(model, args.save_model_path+\"/best_model\")\n",
        "            lrScheduler.step()\n",
        "\n",
        "        print('Finished Training')\n",
        "    else:\n",
        "        print(f\"No training ...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2exmOR6QYVj"
      },
      "source": [
        "Retrieve best model and report the test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drt1xvO_Qa-6"
      },
      "source": [
        "    best_model = torch.load(args.save_model_path+\"/best_model\")\n",
        "    print(f\"Test acc: {get_acc(best_model, testDataloader)}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}